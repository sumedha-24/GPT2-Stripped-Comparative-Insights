{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<torch._C.Generator at 0x135b12390>"
=======
       "<torch._C.Generator at 0x14831a0f0>"
>>>>>>> master
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize wandb\n",
    "# wandb.init(project=\"GPT 2 848K\")\n",
    "# wandb.run.tags = ['GPT 1', 'test run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull from local folder\n",
    "filename = 'tinyshakespeare.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: count how many params you're using in this code, and implement chinchilla law to understand how much data you need to ensure you aren't under training\n",
    "# get vocab\n",
    "vocab = list(sorted(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "# embedding dimensions \n",
<<<<<<< HEAD
    "n_emb = 4 # 32\n",
    "learning_rate = 1e-4\n",
    "epochs = 5000\n",
    "batch_size = 4 # how many sequences we will process in parallel, each of these sequences is block_size long\n",
    "block_size = 4 # 8 # the length of each sequence\n",
=======
    "n_emb = 32\n",
    "learning_rate = 1e-4\n",
    "block_size = 8\n",
    "epochs = 5000\n",
    "batch_size = 4 # how many sequences we will process in parallel, each of these sequences is block_size long\n",
    "block_size = 8 # the length of each sequence\n",
>>>>>>> master
    "# how often to evaluate loss\n",
    "eval_iter = 200\n",
    "# number of blocks in the transformer\n",
    "n_layer = 2\n",
    "# number of heads in the transformer\n",
    "n_heads = 2\n",
    "# each head size is n_emb // n_heads = 32 // 2 = 16\n",
    "dropout = 0.2 # 20% will be zeroed out\n",
    "train_test_split = 0.9 # 85% of data will be used for training\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character level encoding and decoding\n",
    "stoi = {c: i for i, c in enumerate(vocab)}\n",
    "# itos = {i: c for i, c in enumerate(vocab)}\n",
    "# alternate way of creating decoder func\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode full dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# train test split\n",
    "train_size = int(train_test_split * len(data))\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "tensor([ 0, -2])"
=======
       "tensor([  0,  -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20, -22, -24, -26,\n",
       "        -28, -30])"
>>>>>>> master
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.arange(0, n_emb, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "tensor([ 0, -1])"
=======
       "tensor([  0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12, -13,\n",
       "        -14, -15])"
>>>>>>> master
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.arange(0, n_emb // 2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbeddings(nn.Module):\n",
    "    '''Rotary Position Embeddings, as described in the RoPE paper'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = 10_000\n",
    "        theta = torch.pow(self.base, -2 * torch.arange(0, n_emb // 2).float() / n_emb)\n",
    "        pos = torch.arange(block_size, dtype=theta.dtype, device=theta.device)\n",
    "\n",
    "        # Compute position * theta for sin and cos\n",
    "        idx_theta = pos.view(-1, 1) * theta.view(1, -1)\n",
    "\n",
    "        # Precompute sin and cos\n",
    "        cache = torch.stack((torch.cos(idx_theta), torch.sin(idx_theta)), dim=-1)  # Shape: [block_size, n_emb // 2, 2]\n",
    "\n",
    "        # Register the cache as a non-persistent buffer\n",
    "        self.register_buffer('cache', cache, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [b, seq_len, nh, hs].\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Rotated tensor of the same shape as input.\n",
    "        '''\n",
    "        b, seq_len, nh, hs = x.shape  # Extract input dimensions\n",
    "\n",
    "        # Slice the RoPE cache to match the sequence length\n",
    "        rope_cache = self.cache[:seq_len].to(x.device)  # Shape: [seq_len, n_emb // 2, 2]\n",
    "\n",
    "        # Reshape input for rotation (split last dim into pairs)\n",
    "        x = x.reshape(b, seq_len, nh, hs // 2, 2)  # Shape: [b, seq_len, nh, hs // 2, 2]\n",
    "\n",
    "        # Add singleton dimensions to rope_cache for broadcasting\n",
    "        rope_cache = rope_cache.unsqueeze(0).unsqueeze(2)  # Shape: [1, seq_len, 1, h_s // 2, 2]\n",
    "\n",
    "        # Perform the RoPE rotation\n",
    "        rotated = torch.stack([\n",
    "            x[..., 0] * rope_cache[..., 0] - x[..., 1] * rope_cache[..., 1],  # cos * even - sin * odd\n",
    "            x[..., 1] * rope_cache[..., 0] + x[..., 0] * rope_cache[..., 1]   # sin * even + cos * odd\n",
    "        ], dim=-1)  # Shape: [b, seq_len, nh, hs // 2, 2]\n",
    "\n",
    "        # Flatten the last two dimensions back into the original shape\n",
    "        return rotated.flatten(-2).type_as(x)  # Shape: [b, seq_len, nh, hs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9512,  0.3049,  0.3452,  0.1730],\n",
       "          [ 0.0361,  0.0544,  0.8096,  0.5435]],\n",
       "\n",
       "         [[ 0.0283,  0.7180,  0.0111,  0.2392],\n",
       "          [-0.3223,  1.0839,  0.1825,  0.3547]],\n",
       "\n",
       "         [[-0.5386, -0.2178,  0.6250,  0.9900],\n",
       "          [-0.4905,  0.2562,  0.4832,  0.9466]],\n",
       "\n",
       "         [[-0.7186, -0.4412,  0.4733,  0.3417],\n",
       "          [-0.7079, -0.7612,  0.6618,  0.3654]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4658,  0.9040,  0.2983,  0.2226],\n",
       "          [ 0.0893,  0.7697,  0.2282,  0.0069]],\n",
       "\n",
       "         [[ 0.2953,  0.7477,  0.0530,  0.4910],\n",
       "          [-0.5561,  0.3807,  0.2625,  0.3246]],\n",
       "\n",
       "         [[-0.5307,  0.1436,  0.7377,  0.9210],\n",
       "          [-0.7697,  0.0290,  0.0188,  0.2974]],\n",
       "\n",
       "         [[-0.9772, -0.4779,  0.3674,  0.9457],\n",
       "          [-0.7611, -0.0625,  0.2609,  0.9397]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2132,  0.2642,  0.2081,  0.8701],\n",
       "          [ 0.8503,  0.8239,  0.0898,  0.6144]],\n",
       "\n",
       "         [[ 0.4694,  0.7405,  0.0265,  0.4433],\n",
       "          [ 0.3436,  0.5399,  0.0807,  0.4062]],\n",
       "\n",
       "         [[-0.3921,  0.7123,  0.0277,  0.2500],\n",
       "          [-0.3813,  0.0161,  0.9244,  0.2660]],\n",
       "\n",
       "         [[-0.8686, -0.6662,  0.1801,  0.4896],\n",
       "          [-1.0143, -0.2890,  0.3973,  0.5444]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5828,  0.5166,  0.3455,  0.3351],\n",
       "          [ 0.8088,  0.6192,  0.8202,  0.3593]],\n",
       "\n",
       "         [[-0.1286,  0.4692,  0.4154,  0.7517],\n",
       "          [ 0.0529,  0.8306,  0.7389,  0.5570]],\n",
       "\n",
       "         [[-0.5893,  0.6852,  0.9172,  0.2766],\n",
       "          [-1.1178,  0.2734,  0.2209,  0.2421]],\n",
       "\n",
       "         [[-0.9543, -0.7193,  0.5412,  0.8082],\n",
       "          [-0.9227, -0.1775,  0.2121,  0.5096]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope = RotaryPositionEmbeddings()\n",
    "x = torch.rand(batch_size, block_size, n_heads, n_emb)\n",
    "rope(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
=======
   "execution_count": 10,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    '''one head of self-attention'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # usually bias is not used in self-attention TODO: understand better why\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
<<<<<<< HEAD
    "        self.base = 10_000\n",
=======
>>>>>>> master
    "        # triangular mask to prevent attending to future tokens\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        # using register buffer ensures that tril is not initialized as a param, so it won't be optimized during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
<<<<<<< HEAD
    "        self.rope = RotaryPositionEmbeddings()\n",
=======
    "\n",
    "    def create_rotary_embeddings(self):\n",
    "        '''rotary position embedding, as described in the RoPE paper'''\n",
    "        # N is typically set to 10,000 in RoPE implementations\n",
    "        N = 10_000\n",
    "        # Generate base theta values, each value corresponds to a \"pair\" and repeats twice\n",
    "        theta_base = torch.pow(N, -torch.arange(0, n_emb // 2).float() / n_emb)\n",
    "        # Repeat each element in theta_base twice to create the desired pattern\n",
    "        theta = torch.repeat_interleave(theta_base, repeats=2)\n",
    "        pos = torch.arange(block_size).float()\n",
    "        idx_theta = pos[:, None] * theta[None, :]\n",
    "        cache = torch.stack((torch.cos(idx_theta), torch.sin(idx_theta)), dim=-1)\n",
    "        self.register_buffer('cache', cache, persistent=False) # TODO: understand why persistent=False\n",
    "\n",
    "        return theta, pos, idx_theta\n",
>>>>>>> master
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # BxTxC\n",
    "        q = self.query(x) # BxTxC\n",
    "        v = self.value(x) # BxTxC\n",
<<<<<<< HEAD
    "\n",
    "        k = k.view(B, T, n_heads, C // n_heads) # B x T x n_h x h_s\n",
    "        q = q.view(B, T, n_heads, C // n_heads) # B x T x n_h x h_s\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
=======
>>>>>>> master
    "        \n",
    "        # compute attention scores\n",
    "        # could potentially be optimized by using einsum? TODO: understand how\n",
    "        # could potentially use lora's code to optimize this\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5 # BxTxC @ BxCxT (because of transposing second last and last dim of k) --> BxTxT\n",
    "        # BxTxT: the TxT part of this attention matrix is where the quadratic complexity dependent on context length comes from\n",
    "        # * C ** -0.5 is the one over root dk scaling factor in the attention formula\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # wherever tril is 0, in that position of wei, replace existing value with -inf\n",
    "        # :T, :T is sliced to prevent index out of bounds error (for the case where block_size is not equal to T)\n",
    "        wei = torch.softmax(wei, dim=-1) # TODO: understand why we softmax on the last dim\n",
    "        wei = self.dropout(wei) # dropout on attention scores, randomly set some of them to 0\n",
    "        # perform aggregation of values with attention scores\n",
    "        out = wei @ v # BxTxT @ BxTxC --> BxTxC\n",
    "        # out = F.scaled_dot_product_attention(q, k, v, is_causal=True) # BxTxC\n",
    "        # back to the dims we started with\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 4, 2, 2]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m attn \u001b[38;5;241m=\u001b[39m AttentionHead(n_emb \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_heads)\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size, block_size, n_emb)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x) \u001b[38;5;66;03m# BxTxC\u001b[39;00m\n\u001b[1;32m     21\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x) \u001b[38;5;66;03m# BxTxC\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B x T x n_h x h_s\u001b[39;00m\n\u001b[1;32m     24\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(B, T, n_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_heads) \u001b[38;5;66;03m# B x T x n_h x h_s\u001b[39;00m\n\u001b[1;32m     26\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope(q)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 4, 2, 2]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "attn = AttentionHead(n_emb // n_heads)\n",
    "x = torch.rand(batch_size, block_size, n_emb)\n",
    "attn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta.shape, pos.shape, idx.shape\n",
    "# torch.cos(idx).shape"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AttentionHead(n_emb // n_heads)\n",
    "theta, pos, idx = x.create_rotary_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 0.7499, 0.7499, 0.5623, 0.5623, 0.4217, 0.4217, 0.3162,\n",
       "        0.3162, 0.2371, 0.2371, 0.1778, 0.1778, 0.1334, 0.1334, 0.1000, 0.1000,\n",
       "        0.0750, 0.0750, 0.0562, 0.0562, 0.0422, 0.0422, 0.0316, 0.0316, 0.0237,\n",
       "        0.0237, 0.0178, 0.0178, 0.0133, 0.0133])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theta.shape, pos.shape, idx.shape\n",
    "theta"
>>>>>>> master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache"
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 0.7499, 0.7499, 0.5623, 0.5623, 0.4217, 0.4217, 0.3162,\n",
       "        0.3162, 0.2371, 0.2371, 0.1778, 0.1778, 0.1334, 0.1334, 0.1000, 0.1000,\n",
       "        0.0750, 0.0750, 0.0562, 0.0562, 0.0422, 0.0422, 0.0316, 0.0316, 0.0237,\n",
       "        0.0237, 0.0178, 0.0178, 0.0133, 0.0133])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
>>>>>>> master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 14,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''multi headed self attention'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__() # This initializes nn.Module (parent class from which MultiHeadAttention inherits from) before \n",
    "        # initializing anything in this child class\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_emb, n_emb) # linear layer to project concatenated heads output back to n_emb\n",
    "        # project back into the residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # BxTxC\n",
    "        out = self.projection(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    '''simple one layer linear nn'''\n",
    "\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb), # add a factor of 4 to n_emb as per GPT-2, just to make it more expressive, increasing complexity and computation\n",
    "            nn.ReLU(), # TODO: use GELU instead of ReLU\n",
    "            nn.Linear(4 * n_emb, n_emb), # linear projection back into the residual pathway\n",
    "            nn.Dropout(dropout) # add right before connetion before residual connection\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 16,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''transformer block: create multiple blocks and concatenate them'''\n",
    "\n",
    "    def __init__(self, n_emb, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffn = FeedForwardNN(n_emb)\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection # TODO: test using layer norm after sa and ffn as in original transformer paper \n",
    "        # and understand why there was an improvement in the new method\n",
    "        x = x + self.ffn(self.ln2(x)) # residual connection (damn that was a very easy change to make)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 17,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token in the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # W_E in GPT-2\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_emb) # W_P in GPT-2\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, num_heads=n_heads) for _ in range(n_layer)]) # 4 blocks as per GPT-2 \n",
    "        # asterisk is used here to unpack the list of blocks so it can be passed as individual elements to nn.Sequential and not as one big list\n",
    "        # also this is just a simpler representation of the previous thing we did, where we had a list of blocks and we individually called them\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size) # W_o in GPT-2\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
<<<<<<< HEAD
    "        B, T = idx.shape \n",
=======
    "        B, T = idx.shape\n",
>>>>>>> master
    "        # idx and targets are both of shape (batch_size, block_size) aka (B, T)\n",
    "        token_emb = self.token_embedding_table(idx) # Batch x time x channel (here channel is now n_emb)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T)) # time x channel\n",
    "        # x = token_emb + pos_emb  # add positional embedding to token embedding\n",
    "        x = token_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # B, T, vocab size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1)) # we could do this, but its hard to understand, so\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # auto regressive generation\n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx is BxT\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the last block_size tokens of the idx\n",
    "            idx_cond = idx[:, -block_size:] # BxT\n",
    "            logits, loss = self(idx_cond)\n",
    "            # pluck out last column in time dimension, because this is the generated predictions for what comes next\n",
    "            logits = logits[:, -1, :] # keep only the last token for each sequence in the batch aka BxC\n",
    "            probs = F.softmax(logits, dim=-1) # BxC\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # Bx1\n",
    "            # append newly generated token to input idx to obtain new input for next generation iteration\n",
    "            idx = torch.cat([idx, next_token], dim=-1) # Bx(T+1) # TODO: understand why this is dim=-1\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 18,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NanoGPT()\n",
    "idx = torch.randint(vocab_size, (2,4))\n",
    "logits, loss = x(idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "idx"
   ]
=======
   "source": []
>>>>>>> master
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "(idx.reshape(*idx.shape[:-1], -1, 2)).shape"
   ]
=======
   "source": []
>>>>>>> master
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
>>>>>>> master
   "source": [
    "model = NanoGPT()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # TODO: try adding a lr schedule"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track best losses and store losses for plotting\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|          | 27/5000 [00:00<00:42, 116.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 4.302455425262451, Val Loss: 4.40168883562088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|▍         | 225/5000 [00:01<00:27, 175.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Train Loss: 3.3392529487609863, Val Loss: 3.713762534856796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   9%|▉         | 455/5000 [00:02<00:21, 212.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400, Train Loss: 3.524874210357666, Val Loss: 3.321220185756683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|█▎        | 655/5000 [00:02<00:20, 211.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Train Loss: 2.595158100128174, Val Loss: 3.2182006192207337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|█▋        | 842/5000 [00:04<00:26, 155.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 800, Train Loss: 2.9629464149475098, Val Loss: 3.104701887369156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|██        | 1042/5000 [00:05<00:22, 175.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, Train Loss: 2.5057387351989746, Val Loss: 2.9971798968315126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  25%|██▍       | 1238/5000 [00:06<00:22, 168.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1200, Train Loss: 2.911221981048584, Val Loss: 2.9582037484645842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  28%|██▊       | 1412/5000 [00:08<00:46, 77.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1400, Train Loss: 2.732435464859009, Val Loss: 2.8974747002124785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  32%|███▏      | 1615/5000 [00:10<00:46, 73.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1600, Train Loss: 3.0408029556274414, Val Loss: 2.8946908688545228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  37%|███▋      | 1831/5000 [00:11<00:28, 110.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1800, Train Loss: 2.563445806503296, Val Loss: 2.8202089047431946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  41%|████      | 2029/5000 [00:13<00:24, 122.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000, Train Loss: 3.221748113632202, Val Loss: 2.7686537432670595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  45%|████▍     | 2235/5000 [00:14<00:23, 119.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2200, Train Loss: 3.025428056716919, Val Loss: 2.7483159244060515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  49%|████▊     | 2433/5000 [00:16<00:22, 116.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2400, Train Loss: 3.222254514694214, Val Loss: 2.687283924818039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  52%|█████▏    | 2620/5000 [00:17<00:19, 124.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2600, Train Loss: 2.664768695831299, Val Loss: 2.705757224559784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  56%|█████▋    | 2822/5000 [00:18<00:18, 120.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2800, Train Loss: 2.6541106700897217, Val Loss: 2.70195885181427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  61%|██████    | 3030/5000 [00:20<00:15, 126.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000, Train Loss: 2.1677467823028564, Val Loss: 2.65768724322319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  65%|██████▍   | 3237/5000 [00:21<00:13, 126.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3200, Train Loss: 3.0206761360168457, Val Loss: 2.640993736386299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  68%|██████▊   | 3420/5000 [00:22<00:13, 120.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3400, Train Loss: 2.817840814590454, Val Loss: 2.666358770132065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  73%|███████▎  | 3627/5000 [00:24<00:10, 127.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3600, Train Loss: 2.840182065963745, Val Loss: 2.634136815071106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  77%|███████▋  | 3833/5000 [00:25<00:09, 126.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3800, Train Loss: 2.2630693912506104, Val Loss: 2.6102201628684996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  81%|████████  | 4035/5000 [00:26<00:07, 123.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000, Train Loss: 2.642688512802124, Val Loss: 2.6013960182666778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  85%|████████▍ | 4235/5000 [00:28<00:06, 125.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4200, Train Loss: 2.861732006072998, Val Loss: 2.593399704694748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  89%|████████▊ | 4428/5000 [00:29<00:05, 105.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4400, Train Loss: 2.387462854385376, Val Loss: 2.58214755654335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  93%|█████████▎| 4629/5000 [00:31<00:03, 119.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4600, Train Loss: 2.1232218742370605, Val Loss: 2.619614409208298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  97%|█████████▋| 4835/5000 [00:32<00:01, 125.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4800, Train Loss: 2.7502715587615967, Val Loss: 2.6101433753967287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 5000/5000 [00:33<00:00, 149.31it/s]\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "for iter in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, train_loss = model(xb, yb)\n",
    "\n",
    "    # Zero gradients, backward pass, and optimizer step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    # Evaluation phase every eval_iter\n",
    "    if iter % eval_iter == 0:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_losses_list = []\n",
    "\n",
    "        for _ in range(eval_iter):\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                X_val, Y_val = get_batch('val')\n",
    "                logits, val_loss = model(X_val, Y_val)\n",
    "                val_losses_list.append(val_loss.item())\n",
    "        \n",
    "        # Calculate mean of validation losses\n",
    "        avg_val_loss = sum(val_losses_list) / len(val_losses_list)\n",
    "\n",
    "        # Log and print average train and validation losses\n",
    "        print(f\"Epoch: {iter}, Train Loss: {train_loss.item()}, Val Loss: {avg_val_loss}\")\n",
    "        # wandb.log({\n",
    "        #     'train_loss': train_loss.item(),\n",
    "        #     'val_loss': avg_val_loss\n",
    "        # })\n",
    "\n",
    "        # Track best losses\n",
    "        if train_loss.item() < best_train_loss:\n",
    "            best_train_loss = train_loss.item()\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "train_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "print(100*'*')\n",
    "# Load best losses from JSON file if it exists\n",
    "best_losses_file = 'best_losses.json'\n",
    "try:\n",
    "    with open(best_losses_file, 'r') as f:\n",
    "        best_losses = json.load(f)\n",
    "        best_train_loss = best_losses.get('best_train_loss', best_train_loss)\n",
    "        best_val_loss = best_losses.get('best_val_loss', best_val_loss)\n",
    "except FileNotFoundError:\n",
    "    best_losses = {\n",
    "        'best_train_loss': best_train_loss,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    with open(best_losses_file, 'w') as f:\n",
    "        json.dump(best_losses, f)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "Sroko tes nRhol:\n",
      "S berme pllresepy? ollopy'de thount ary Thangic the, handes the maousd t save frkofEIs,\n",
      "The nocore I witisohe l hainss r,\n",
      "E flldiO,\n",
      "Gomeshoulr th fimewenchdoly telelu n? cvourteve t Werotho y e laxr'nd cc.\n",
      " x.\n",
      "A,n meno lobld:r ces berorele hiveho f t?\n",
      "Hhe.\n",
      "Jn hed b niyharo ave d pe\n",
      "!anortinfory tedlhour s.\n",
      ":\n",
      "\n",
      "Nhe\n",
      " 'gall t'd bef wanorehe,nngou th, iske ia\n",
      "NreniSps heem, tindle ve, coanut,\n",
      "Su a k bather canf, hde t fars, erobe,\n",
      "TE.\n",
      "\n",
      "DLYe.:\n",
      "MBast the t ayetErene foes,h ie oser t thar nrgolt athe thoutowensurararof s Er.\n",
      "Dzrve, pis, ives ngan tH:\n",
      "Whoug.\n",
      "\n",
      "D oIAC\n",
      "Thos cG sheaket's nginn:\n",
      "Ix\n",
      "S:\n",
      "OWand-- t ve anthat igt clealleD? ot we  anThecaresthses, a\n",
      "KL-epo win pe ' s'dors Rurhek\n",
      "Ado f Jturll y isiworr: chte berraany t singig Gos: s ausFo tall.\n",
      "Ds pe, bolou r rend mit,\n",
      "We f-?it!\n",
      "T;\n",
      "\n",
      "IKACO'do q'd asasthef :\n",
      "Whesus; trirceali, t llaor, y t yot hou hod r qereat, r\n",
      "ESdom:\n",
      "L\n",
      "?INGano f I thandiouc, te tharshok, be,\n",
      "HB\n",
      "Am theiy avem ash n, y LAgthiu tr'sd gredy funs;\n",
      "Matt.\n",
      "Xmelenoatior he'tpitas S: n nee? orshet shesre  co tHl,\n",
      "KL\n",
      "To VT'dos l y I fnaiteo I  Vore mim.\n",
      "\n",
      "Nl.med mintha-\n",
      "UGO, haanp chesdoed tEr wimaelrs aleco KEO' u por-lrad.y\n",
      "IY\n",
      "Tgoquth t,\n",
      "Avir be te he ' eme t'bnom we thengrur fiveg boWo he, r ntou sheverae, g asdurere thad helauntot gthee fr'golk,\n",
      "\n",
      "Ha f t hitth rcetot t merertewil.\n",
      "To; tce'ee f FwenUnirlllrpoilloie,, y beragr g'darnth b.\n",
      "athabry d youroar ate y y; hyo bfhey rEU\n",
      "\n",
      "\n",
      "IALRHFENADKAw\n",
      "WGUIasgonvIOO, jG:\n",
      "Ianourrt tcRuis y\n",
      "TowiYase .\n",
      "Ybee il!\n",
      "EMA?\n",
      "A?Tkoom Iun os alepis lorius aI hr necof hemrtheri ;I:\n",
      "\n",
      "CUu; wh itond n ithesiecoyolf I:\n",
      "Bnme'd,\n",
      "M\n",
      "IODhocC-\n",
      "Anou\n",
      "Wh.\n",
      "Rgot s wrhiwoce myor, Oint loveand whhoutilngr?Ay loure et hoke thaverl.\n",
      "\n",
      "Fo,\n",
      "AMDbsX hind b ch\n",
      "Aqote I maniseat bit sturmey amiverie ms tound ath ay, aye athashea oisiur mther he theundFou tngorst grge hama\n",
      "\n",
      "MAnAde h'd isan:\n",
      "Tous m winde C\n",
      "Gde, tho:\n",
      "Aq these f, f hese sert anand prony sel.\n",
      "\n",
      "W:\n",
      "'n's\n",
      ":\n",
      "F helelaitm pow'decu I tho i,re sore tais t fhes boreser th Calis,\n",
      "We llancmeonse\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "print(f\"Generated Text:\")\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_text = decode(model.generate(idx, max_new_tokens=2000)[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run did not beat the best losses. Generated text not saved.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "# Check if current run has better losses\n",
    "if best_train_loss < best_losses.get('best_train_loss', float('inf')) or best_val_loss < best_losses.get('best_val_loss', float('inf')):\n",
    "    # Save generated text to file\n",
    "    with open('generated_shakespeare_text.txt', 'w') as f:\n",
    "        f.write(generated_text)\n",
    "\n",
    "    # Update best losses and save to JSON file\n",
    "    best_losses['best_train_loss'] = best_train_loss\n",
    "    best_losses['best_val_loss'] = best_val_loss\n",
    "    with open(best_losses_file, 'w') as f:\n",
    "        json.dump(best_losses, f)\n",
    "\n",
    "    # Have wandb save the text file\n",
    "    wandb.save('generated_shakespeare_text.txt')\n",
    "    # also save an image of the training and validation loss curves\n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='val loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('train_val_loss.png')\n",
    "    wandb.save('train_val_loss.png')\n",
    "    print(\"Current run beat the best losses. Generated text saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"Current run did not beat the best losses. Generated text not saved.\")\n",
    "print(100*'*')\n",
    "print(100*'*')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Train Loss: 2.1232218742370605\n",
      "Best Validation Loss: 2.58214755654335\n",
      "Total number of parameters in the model: 29697\n",
      "Total number of tokens in the dataset: 1115394\n",
      "According to Chinchilla Law, you need at least 59394 tokens to train this model.\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "print(f\"Best Train Loss: {best_train_loss}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")\n",
    "# show total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")\n",
    "# show toal number of tokens in the dataset\n",
    "total_tokens = len(data)\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n",
    "print(f\"According to Chinchilla Law, you need at least {total_params * 2} tokens to train this model.\") # TODO: work on this"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train model up to 5000 epochs: 33.50 seconds\n"
     ]
    }
   ],
>>>>>>> master
   "source": [
    "# Ensure train_time and other parameters are defined before logging\n",
    "# wandb.log({\n",
    "#     'epochs': epochs,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"block_size\": block_size,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"embedding_size\": n_emb,\n",
    "#     \"optimizer\": \"AdamW\",\n",
    "#     \"device\": device,\n",
    "#     \"vocab_size\": vocab_size,\n",
    "#     \"best_train_loss\": best_train_loss,\n",
    "#     \"best_val_loss\": best_val_loss,\n",
    "#     'Training Time': train_time, \n",
    "#     'dropout': dropout,\n",
    "#     'n_layer': n_layer,\n",
    "#     'n_heads': n_heads,\n",
    "#     'train_test_split': train_test_split,\n",
    "#     'total_params': total_params\n",
    "# })\n",
    "\n",
    "print(f\"Total time to train model up to {epochs} epochs: {train_time:.2f} seconds\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> master
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
