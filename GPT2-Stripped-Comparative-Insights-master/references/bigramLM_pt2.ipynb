{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets build a name generator, but this time use more than one previous character as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull text file\n",
    "names = open('names.txt').read().splitlines()\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "# create vocab list\n",
    "vocab = sorted(list(set(''.join(names))))\n",
    "\n",
    "# create tokenizer encoder\n",
    "stoi = {}\n",
    "# make the model hallucinate a start token so that we can propmt it to generate a name\n",
    "# stoi is String TO Integer\n",
    "stoi['.'] = 0\n",
    "stoi.update({s:i+1 for i, s in enumerate(vocab)})\n",
    "\n",
    "# create tokenizer decoder\n",
    "# itos is Integer TO String\n",
    "itos = {}\n",
    "# make the model hallucinate an end token so that it knows when to end the name during generation\n",
    "itos[0] = '.'\n",
    "itos.update({i+1:s for i, s in enumerate(vocab)})\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2\n",
    "\n",
    "def create_splits(names, stoi, block_size, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    def build_dataset(names):\n",
    "        X, Y = [], []\n",
    "        for name in names:\n",
    "            context = [0] * block_size\n",
    "            name += '.'\n",
    "            for ch in name:\n",
    "                X.append(context)\n",
    "                Y.append(stoi[ch])\n",
    "                context = context[1:] + [stoi[ch]]\n",
    "        return torch.tensor(X), torch.tensor(Y)\n",
    "    \n",
    "    n = len(names)\n",
    "    train_end = int(split_ratios[0] * n)\n",
    "    val_end = train_end + int(split_ratios[1] * n)\n",
    "    \n",
    "    train_names = names[:train_end]\n",
    "    val_names = names[train_end:val_end]\n",
    "    test_names = names[val_end:]\n",
    "    \n",
    "    X_train, Y_train = build_dataset(train_names)\n",
    "    X_val, Y_val = build_dataset(val_names)\n",
    "    X_test, Y_test = build_dataset(test_names)\n",
    "    \n",
    "    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)\n",
    "\n",
    "# Example usage\n",
    "(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = create_splits(names, stoi, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([182778, 2])\n",
      "Y_train shape: torch.Size([182778])\n",
      "X_val shape: torch.Size([22633, 2])\n",
      "Y_val shape: torch.Size([22633])\n",
      "X_test shape: torch.Size([22735, 2])\n",
      "Y_test shape: torch.Size([22735])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"Y_val shape:\", Y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what does it mean when embedding (emb) matrix is of shape: [n, 3, 2]\n",
    "### there are n training examples, each of which has three characters (block size). and each character is represented (squashed down) to 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding dot product with a matrix is equivalent to plucking out a single row. this can be thought of a first layer of the network where we obtain the embeddings for each tokenized vector. for now we'll settle with pytorch slicing which also allows this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3481\n"
     ]
    }
   ],
   "source": [
    "# initialize params\n",
    "C = torch.randn((27, 2))\n",
    "W1 = torch.randn((6, 100)) # 6 because we give 3 characters at a time, and each has 2 numbers to represent them. so 6 in all\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [W1, b1, W2, b2, C]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a range of learning rates separated by degrees of 10\n",
    "steps = torch.linspace(-3, 0, 1000)\n",
    "lri = 10 ** steps\n",
    "lri[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 2, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (20) to match target batch_size (30).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# probs = torch.softmax(logits, 1) # softmax along dimension 1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# loss = -torch.log(probs[torch.arange(X.shape[0]), Y]).mean() # calculate NLL for all X.shape[0] examples (akin to batch size)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# does the same thing the above two lines does\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-AI/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (20) to match target batch_size (30)."
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    # fetch batch size = 32 number of indices to mini batch the data\n",
    "    ix = torch.randint(0, X_train.shape[0], (32,))\n",
    "    ix\n",
    "    # forward pass\n",
    "    emb = C[X_train[ix]]\n",
    "    print(emb.shape)\n",
    "    # layer 1\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    # layer 2\n",
    "    logits = h @ W2 + b2\n",
    "    # probs = torch.softmax(logits, 1) # softmax along dimension 1\n",
    "    # loss = -torch.log(probs[torch.arange(X.shape[0]), Y]).mean() # calculate NLL for all X.shape[0] examples (akin to batch size)\n",
    "    loss = F.cross_entropy(logits, Y_train[ix]) # does the same thing the above two lines does\n",
    "    # backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # gradient update\n",
    "    for p in parameters:\n",
    "        p.data += -lri[i]*p.grad\n",
    "    losses.append(loss.item())\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch: {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lri, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this plot above is made for experimental purposes. we see that there is a sweet spot for the learning rate where loss is at the all time low. a high leraning rate leads to divergence, and a lor learning rate moves the gradient way too slowly. this is what the graph conveys. \n",
    "### so now we've found a decent learning rate of around 0.1. lets train on that, and once we see the loss plateauing, we'll decay the learning rate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "C = torch.randn((27, 2))\n",
    "W1 = torch.randn((6, 100)) # 6 because we give 3 characters at a time, and each has 2 numbers to represent them. so 6 in all\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [W1, b1, W2, b2, C]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    # fetch batch size = 32 number of indices to mini batch the data\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    ix\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    # layer 1\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    # layer 2\n",
    "    logits = h @ W2 + b2\n",
    "    # probs = torch.softmax(logits, 1) # softmax along dimension 1\n",
    "    # loss = -torch.log(probs[torch.arange(X.shape[0]), Y]).mean() # calculate NLL for all X.shape[0] examples (akin to batch size)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # does the same thing the above two lines does\n",
    "    # backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # gradient update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "# layer 1\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "# layer 2\n",
    "logits = h @ W2 + b2\n",
    "# probs = torch.softmax(logits, 1) # softmax along dimension 1\n",
    "# loss = -torch.log(probs[torch.arange(X.shape[0]), Y]).mean() # calculate NLL for all X.shape[0] examples (akin to batch size)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we train for 10k epochs, we notice that the loss starts to plateau at 2.37, so lets drop the learning rate. once we drop it to 0.01, we see that the loss drop to 2.31, lesgooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat((emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]), 1).shape # --> this doesn't work when we have a bigger batch size\n",
    "# torch.cat(torch.unbind(emb, 1), 1).shape # --> this works better, we unbind the 1st dimension and then concatenate the 1st dimension to match 6 with 6 from the next layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the addition between emb.w1 and b works correctly without additional editing because:\n",
    "emb.w1 shape: 32, 100\n",
    "b1 shape:         100\n",
    "pytorch broadcasting rules matches the right dim, then fills the left ones with 1 if its missing, and then 1 is broadcasted to match whatever is above it\n",
    "b1 broadcasted shape: 1, 100 to 32, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
